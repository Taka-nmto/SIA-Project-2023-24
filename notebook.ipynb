{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automating data querying with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lucie Le Rolland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: async-generator==1.10 in /home/lucie/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 7)) (1.10)\n",
      "Requirement already satisfied: attrs==22.2.0 in /home/lucie/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 9)) (22.2.0)\n",
      "Requirement already satisfied: beautifulsoup4==4.11.2 in /home/lucie/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 13)) (4.11.2)\n",
      "Requirement already satisfied: bs4==0.0.1 in /home/lucie/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 15)) (0.0.1)\n",
      "Requirement already satisfied: certifi==2022.12.7 in /home/lucie/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 17)) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer==3.1.0 in /home/lucie/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 21)) (3.1.0)\n",
      "Requirement already satisfied: exceptiongroup==1.1.1 in /home/lucie/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 23)) (1.1.1)\n",
      "Requirement already satisfied: h11==0.14.0 in /home/lucie/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 27)) (0.14.0)\n",
      "Requirement already satisfied: idna==3.4 in /home/lucie/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 29)) (3.4)\n",
      "Requirement already satisfied: outcome==1.2.0 in /home/lucie/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 33)) (1.2.0)\n",
      "Requirement already satisfied: pysocks==1.7.1 in /home/lucie/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 35)) (1.7.1)\n",
      "Requirement already satisfied: requests==2.28.2 in /home/lucie/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 37)) (2.28.2)\n",
      "Requirement already satisfied: selenium==4.8.2 in /home/lucie/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 39)) (4.8.2)\n",
      "Requirement already satisfied: sniffio==1.3.0 in /home/lucie/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 41)) (1.3.0)\n",
      "Requirement already satisfied: sortedcontainers==2.4.0 in /home/lucie/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 43)) (2.4.0)\n",
      "Requirement already satisfied: soupsieve==2.4 in /home/lucie/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 45)) (2.4)\n",
      "Requirement already satisfied: trio==0.22.0 in /home/lucie/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 47)) (0.22.0)\n",
      "Requirement already satisfied: trio-websocket==0.10.0 in /home/lucie/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 51)) (0.10.0)\n",
      "Requirement already satisfied: urllib3[socks]==1.26.15 in /home/lucie/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 53)) (1.26.15)\n",
      "Requirement already satisfied: wsproto==1.2.0 in /home/lucie/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 57)) (1.2.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction and disclaimer\n",
    "\n",
    "In this session, we are going to fetch data from remote sources, whether they are structured to be queried or a bit less so. \n",
    "\n",
    "One thing to always keep in mind is that somebody has to maintain the service and/or is paying for the bandwidth you're using for these queries. Always be considerate when querying an API or scraping a website. Make sure you're not asking for way more data than you need, and always remember that the following lines of code are your friends:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "time.sleep(1)  # Let's take a quick snooze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if you don't particularly care for the provider you're querying, remember that while a couple of well-timed requests won't alarm anyone, a sudden and brazen spike in requests may attract unwanted attention. \n",
    "\n",
    "It also goes without saying that before scraping a website, you should check its terms of services to make sure that it's legal to do so. Before going through the trouble (or fun) of scraping the website, you can also just ask the owner if they'd be keen on sharing their data with you. Some might!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting data from the web with requests and BeautifulSoup\n",
    "\n",
    "### What is a webpage ?\n",
    "\n",
    "Let's first take a few minutes to remind you of how HTML works. If you need in-depth information, there are plenty of guides available online.\n",
    "\n",
    "- HTML is the language in which simple web pages are written. You can look up the HTML code of every page you're browsing by clicking right anywhere on the page and click on \"Inspect\". \n",
    "- Information is organised through opening and closing tags (`<p>A paragraph</p>`, e.g.). Among the most common, `<p>` denotes a paragraph, `<h1>...<hn>` are headers, `<ul>` is a list where each entry is a `<li>`. \n",
    "- Some of these tags have attributes, which sometimes help selecting them in the page\n",
    "- When you load a page, what your browser does is basically a GET API call. You can replicate it with requests. \n",
    "\n",
    "When scraping, the general strategy is to read the HTML code of you page with inspect, understand its structure, then get the whole HTML with requests and extract the elements you need from it. \n",
    "\n",
    "Scraping is an artisanal process that relies on trial and error. There isn't generally one true way to extract the data you need. You need to balance the amount of effort you put into writing the code with how reusable you need it to be, and how well you need it to generalize to new pages.\n",
    "\n",
    "For this reason, when you scrape a big corpus (several hundreds or thousands of different pages), where the time.sleep() constraint weighs on the time it takes to build the dataset, it's generally not a bad idea to save the HTMLs as you go. This way, if it turns out that your code didn't anticipate a specific feature that appears on some pages, you won't have to start all over again.\n",
    "\n",
    "### Parsing HTML with BeautifulSoup\n",
    "\n",
    "That extraction phase is where BeautifulSoup comes along. It does quite a bit of invisible stuff to help you, such as fixing broken HTML - which used to plague the web - and dealing with encodings. It also helps you navigate the HTML by building it as a tree. [This section of the documentation](https://beautiful-soup-4.readthedocs.io/en/latest/#searching-the-tree) contains a lot of information to properly look things up in your HTML. \n",
    "\n",
    "Let's look up an example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_url = \"https://www.w3docs.com/learn-html/table-of-html-tags.html\"\n",
    "\n",
    "list_of_tags = requests.get(scraped_url)\n",
    "\n",
    "with open('list_of_tags.html', 'w') as file:  # This is how you'd save the file\n",
    "    file.write(list_of_tags.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "souped_list_of_tags = BeautifulSoup(list_of_tags.text)\n",
    "\n",
    "# Alternatively, we could read the html we saved\n",
    "\n",
    "with open('list_of_tags.html', 'r') as file:\n",
    "    souped_read_list_of_tags = BeautifulSoup(file.read())\n",
    "\n",
    "list_of_tag_tables = []\n",
    "\n",
    "for table in souped_list_of_tags.find_all('table'):    # We look up all tables\n",
    "    headers = [header.string for header in table.find_all('th')]   # First let's extract the headers\n",
    "    entries = {header: [] for header in headers}\n",
    "    table_body = table.find('tbody')  # find looks up just the first element that matches the criterion\n",
    "    for line in table_body.find_all('tr'):\n",
    "        for cell_number in range(len(headers)):\n",
    "            entries[headers[cell_number]].append(line.find_all('td')[cell_number].text)\n",
    "\n",
    "    list_of_tag_tables.append(pd.DataFrame(entries))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Tag                                        Description Descriptions\n",
      "0  <!DOCTYPE>                     Sets the type of the document.          NaN\n",
      "1      <html>                            Sets an HTML document.           NaN\n",
      "2      <head>  Contains general information (metadata) about ...          NaN\n",
      "3     <title>                     Sets a title of the document.           NaN\n",
      "4      <body>                Specifies the body of the document.          NaN\n",
      "           Tag                                        Description\n",
      "0   <!DOCTYPE>                     Sets the type of the document.\n",
      "1       <html>                            Sets an HTML document. \n",
      "2       <head>  Contains general information (metadata) about ...\n",
      "3      <title>                     Sets a title of the document. \n",
      "4       <body>                Specifies the body of the document.\n",
      "..         ...                                                ...\n",
      "1   <noscript>  Defines an alternate content to be displayed i...\n",
      "2     <applet>  Specifies an embedded applet. Not supported in...\n",
      "3      <embed>  \\r\\nContains external application, typically m...\n",
      "4     <object>  \\r\\nDefines an embedded object (video, Flash, ...\n",
      "5      <param>  \\r\\nDefines a parameter for an object or apple...\n",
      "\n",
      "[123 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "list_of_tags_df = pd.concat(list_of_tag_tables)\n",
    "print(list_of_tags_df.head())  # A bit annoying : one of the table has a \"Descriptions\" column rather than \"Description\"\n",
    "\n",
    "for table in list_of_tag_tables:\n",
    "    table.rename({\"Descriptions\": \"Description\"}, axis = 1, inplace = True)\n",
    "\n",
    "list_of_tags_df = pd.concat(list_of_tag_tables)\n",
    "\n",
    "print(list_of_tags_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what if I weren't interested in all the table, but just the basic tags? Then we'd need to only look up a single table.\n",
    "\n",
    "It's a bit tricky because all tables seem to have the same attributes. The only leverageable thing seems to be the header right before. This is where this all becomes quite artisanal! (And you may have better ideas or ones that I didn't see!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tag</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;!DOCTYPE&gt;</td>\n",
       "      <td>Sets the type of the document.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;html&gt;</td>\n",
       "      <td>Sets an HTML document.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;head&gt;</td>\n",
       "      <td>Contains general information (metadata) about ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;title&gt;</td>\n",
       "      <td>Sets a title of the document.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;body&gt;</td>\n",
       "      <td>Specifies the body of the document.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\\n&lt;h1&gt; to &lt;h6&gt;\\n</td>\n",
       "      <td>Defines \\r\\nHTML headings.\\r\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>&lt;p&gt;</td>\n",
       "      <td>Defines a paragraph.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>&lt;br&gt;</td>\n",
       "      <td>Specifies a line break.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>&lt;hr&gt;</td>\n",
       "      <td>Inserts a horizontal line or defines a themati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>&lt;!-- ... --&gt;</td>\n",
       "      <td>Defines a comment.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Tag                                        Description\n",
       "0        <!DOCTYPE>                     Sets the type of the document.\n",
       "1            <html>                            Sets an HTML document. \n",
       "2            <head>  Contains general information (metadata) about ...\n",
       "3           <title>                     Sets a title of the document. \n",
       "4            <body>                Specifies the body of the document.\n",
       "5  \\n<h1> to <h6>\\n                     Defines \\r\\nHTML headings.\\r\\n\n",
       "6               <p>                               Defines a paragraph.\n",
       "7              <br>                           Specifies a line break. \n",
       "8              <hr>  Inserts a horizontal line or defines a themati...\n",
       "9     <!-- ... -->                                  Defines a comment."
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_tags_html = souped_list_of_tags.find('h2', id='basic-tags-2').next_sibling.next_sibling\n",
    "\n",
    "headers = [header.string for header in basic_tags_html.find_all('th')]   # First let's extract the headers\n",
    "entries = {header: [] for header in headers}\n",
    "table_body = basic_tags_html.find('tbody')  # find looks up just the first element that matches the criterion\n",
    "for line in table_body.find_all('tr'):\n",
    "    for cell_number in range(len(headers)):\n",
    "        entries[headers[cell_number]].append(line.find_all('td')[cell_number].text)\n",
    "        \n",
    "basic_tags_df = pd.DataFrame(entries)\n",
    "basic_tags_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automating navigation on a website with Selenium\n",
    "\n",
    "In its quest to scrape as much HTML as possible, requests has one enemy: interactivity. Basically, all the things that happen after you load the page and that you can interact with (such as all this pesky Javascript) are not attainable with requests.\n",
    "\n",
    "Let's look an example (courtesy of a student from last year, happy to shift source if you have an example). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "commercial_registry = 'https://cres.gov.ai/bereg/searchbusinesspublic#'\n",
    "\n",
    "commercial_registry_html = BeautifulSoup(requests.get(commercial_registry).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<html><body><p>ï»¿<!DOCTYPE html>\n",
       "\n",
       "</p>\n",
       "<meta charset=\"utf-8\"/>\n",
       "<script src=\"js/base.js?_=13\" type=\"application/javascript\"></script>\n",
       "<title>Anguilla Commercial Registry</title>\n",
       "<meta content=\"description\" name=\"description\"/>\n",
       "<meta content=\"DevOOPS\" name=\"author\"/>\n",
       "<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n",
       "<link href=\"css/base.css?_=13\" rel=\"stylesheet\"/>\n",
       "<link href=\"css/all.css?_=2962\" rel=\"stylesheet\"/>\n",
       "<link href=\"apple-icon-57x57.png\" rel=\"apple-touch-icon\" sizes=\"57x57\"/>\n",
       "<link href=\"apple-icon-60x60.png\" rel=\"apple-touch-icon\" sizes=\"60x60\"/>\n",
       "<link href=\"apple-icon-72x72.png\" rel=\"apple-touch-icon\" sizes=\"72x72\"/>\n",
       "<link href=\"apple-icon-76x76.png\" rel=\"apple-touch-icon\" sizes=\"76x76\"/>\n",
       "<link href=\"apple-icon-114x114.png\" rel=\"apple-touch-icon\" sizes=\"114x114\"/>\n",
       "<link href=\"apple-icon-120x120.png\" rel=\"apple-touch-icon\" sizes=\"120x120\"/>\n",
       "<link href=\"apple-icon-144x144.png\" rel=\"apple-touch-icon\" sizes=\"144x144\"/>\n",
       "<link href=\"apple-icon-152x152.png\" rel=\"apple-touch-icon\" sizes=\"152x152\"/>\n",
       "<link href=\"apple-icon-180x180.png\" rel=\"apple-touch-icon\" sizes=\"180x180\"/>\n",
       "<link href=\"android-icon-192x192.png\" rel=\"icon\" sizes=\"192x192\" type=\"image/png\"/>\n",
       "<link href=\"favicon-32x32.png\" rel=\"icon\" sizes=\"32x32\" type=\"image/png\"/>\n",
       "<link href=\"favicon-96x96.png\" rel=\"icon\" sizes=\"96x96\" type=\"image/png\"/>\n",
       "<link href=\"favicon-16x16.png\" rel=\"icon\" sizes=\"16x16\" type=\"image/png\"/>\n",
       "<link href=\"manifest.json\" rel=\"manifest\"/>\n",
       "<meta content=\"#ffffff\" name=\"msapplication-TileColor\"/>\n",
       "<meta content=\"ms-icon-144x144.png\" name=\"msapplication-TileImage\"/>\n",
       "<meta content=\"#ffffff\" name=\"theme-color\"/>\n",
       "<style>\n",
       "\t.green-box {\n",
       "\tbackground-color:#12c17d;\n",
       "\t}\n",
       "\t.card {\n",
       "\t  display: inline-block;\n",
       "\t  vertical-align: middle\n",
       "\t}\n",
       "\t.logo-wrapper {\n",
       "\t  width: 100%;\n",
       "\t  display: inline-block;\n",
       "\t}\n",
       "\t\n",
       "\t.logo-img,\n",
       "\t.logo-text {\n",
       "\t  display: inline-block;\n",
       "\t  vertical-align: middle;\n",
       "\t  font-size: 20px;\n",
       "\t}\n",
       "\n",
       "    .header-label{\n",
       "        display: inline-block;\n",
       "        max-width: 100%;\n",
       "        margin-bottom: 5px;\n",
       "        font-weight: bold;\n",
       "        position: absolute;\n",
       "        Z-INDEX: 1000;\n",
       "    }\n",
       "\t</style>\n",
       "<!-- theme-oscs -->\n",
       "<!--Start Header-->\n",
       "<header class=\"navbar\">\n",
       "<div class=\"container-fluid expanded-panel\">\n",
       "<div class=\"row\">\n",
       "<div class=\"col-lg-5 col-md-6 col-md-offset-1\">\n",
       "<div class=\"logo-wrapper\">\n",
       "<div class=\"logo-img\">\n",
       "<a href=\"#\" style=\"text-decoration: none; color: #525252; height: 75px;\">\n",
       "<img alt=\"logo\" src=\"img/cres-min.png\" style=\"display: inline-block; height: 75px;\"/>\n",
       "</a>\n",
       "</div>\n",
       "<div class=\"logo-text hidden-xs\">\n",
       "<p style=\"margin:0;\">ANGUILLA COMMERCIAL REGISTRY</p>\n",
       "</div>\n",
       "</div>\n",
       "</div>\n",
       "</div>\n",
       "</div>\n",
       "</header>\n",
       "<!--End Header-->\n",
       "<!--Start Frame-->\n",
       "<div class=\"container-fluid single-form\" id=\"main\"><div class=\"row\" id=\"_fr\"></div></div>\n",
       "<!--End Frame-->\n",
       "<div id=\"csrf\"><input name=\"NCSRF\" type=\"hidden\" value=\"RandomBytes#mao6EEt+sNKoHQ==|Hmac#Iuya7ZZ+e1PJ74utKEexROxH/dhEh4juPoZhGIONa1c=|CreatedDate#2023-03-17T05:50:04.4499893-04:00\"/></div>\n",
       "<script src=\"js/lib.js?_=2962\" type=\"application/javascript\"></script>\n",
       "<script src=\"res/resources.js?_=2962&amp;r=308111158\" type=\"application/javascript\"></script>\n",
       "<script type=\"application/javascript\">\n",
       "\n",
       "          ï»¿$Forms.SearchBusinessPublic=$t.frm.Search(\"SearchBusinessPublic\",\"Search for a Business Entity\").extend({buttons:function(a){return(a||(typeof a===\"undefined\"))?[$t.btn.success(\"filter\",\"search\",\"Search\").extend({click:function(c){c.form.filter();c._gidx.groups.byid.$filter.commit()}}),$t.btn.clear(),$t.btn.back()]:[]},back:function(){location.href=\"../um/login\"},filterGroup:function(){return $t.grp.group(\"filterCtl\",[$t.grp.ialert(\"info_text_name_publicSearch\",\"IMPORTANT: Search is performed only among Business Entities registered in Anguilla Commercial Registry. Search by Certificate No.can be successful only if Certificate was issued via Commercial Registry Electronic System. To request information about Business Entities whose Certificates were issued earlier than May of 2022 please contact Anguilla Commercial Registry Office.\"),$t.grp.horz(\"g1\",null,[$t.grp.group(\"c1\",[$t.fld.select(\"business_entity_type\",\"Business Entity Type\",$res.BETYPE),{id:\"reg_number\",type:\"text\",label:\"Registration No. / Deposit No.\"}]),$t.grp.group(\"c2\",[{id:\"name\",type:\"text\",label:\"Business Entity Name\"},{id:\"cert_number\",type:\"text\",label:\"Certificate No.\"}])]),$t.fld.jq(\"line_break\",function(a){return $(\"</br>\")}),])},listCtrl:function(){return{type:\"jtable\",id:\"list\",nolabel:true,table:{actions:{listUrl:\"list/public/business.json\"},columns:{reg_number:$t.col.str(\"Registration No. / Deposit No.\",99,\"5%\"),reg_date:$t.col.date(\"Registration Date\"),name:$t.col.str(\"Business Entity Name\",99,\"15%\"),foreign_name:$t.col.str(\"Foreign Character Name\",99,\"15%\"),business_entity_type_str:$t.col.str(\"Business Entity Type\",99,\"5%\"),business_entity_subtype:$t.col.str(\"Business Entity Category\",99,\"10%\"),reg_address:$t.col.str(\"Registered Office\",250,\"20%\"),annual_return_year:$t.col.str(\"Last Annual Return / Fee\",99,\"3%\"),state:$t.col.str(\"Status\",50,\"5%\"),institution_str:$t.col.str(\"Representative Institution\",250,\"15%\")}}}}});\n",
       "        $(document).ready(function () {\n",
       "            $Ctl.rootForm(new $Forms.SearchBusinessPublic());\n",
       "        });\n",
       "\n",
       "    </script>\n",
       "</body></html>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "commercial_registry_html   # The table is not there!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thankfully, there's a solution to that problem: Selenium. This module, that was primarily developed to test web applications, can also be leveraged to scrape some precious data. \n",
    "\n",
    "Selenium basically emulates a browser. The first thing you need to do before using Selenium is to install a driver that will mimic one. There are [drivers for all major web browsers](https://selenium-python.readthedocs.io/installation.html#drivers). In this class, we'll use Firefox, but some pages may not be compatible with all drivers so keep in mind that you can switch them up. Each OS has their own driver, I've put compressed versions for the three main OS, let's first install this.\n",
    "\n",
    "Selenium allows you to find HTML objects that are the results of an interaction, and either investigate their contents or interact with them (click on something, enter some text in a field, etc). Let's first watch Selenium extract data using the same example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "driver = webdriver.Firefox()\n",
    "driver.get(commercial_registry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example is very similar to what we did with the html tag example \n",
    "# I even copied, pasted and slightly modified some snippets from above to match the find_element vs find_elements\n",
    "# structure of Selenium, and the (By.(something), 'something') logic.\n",
    "\n",
    "table = driver.find_element(By.TAG_NAME, 'table')\n",
    "\n",
    "headers = [header.text for header in table.find_elements(By.TAG_NAME, 'th')]\n",
    "entries = {header: [] for header in headers}\n",
    "\n",
    "table_body = table.find_element(By.TAG_NAME, 'tbody')\n",
    "\n",
    "for lines in table_body.find_elements(By.TAG_NAME, 'tr'):\n",
    "    for cell_number in range(len(headers)):\n",
    "        entries[headers[cell_number]].append(lines.find_elements(By.TAG_NAME, 'td')[cell_number].text)\n",
    "\n",
    "entries_df = pd.DataFrame(entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if requests had been able to read the table, it wouldn't have been able to turn the pages on the website. With Selenium, you can do that. Let's find this button and click it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_turner = driver.find_element(By.CLASS_NAME, \"jtable-page-number-next\").click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we're on page 2! \n",
    "\n",
    "With a well-constructed loop, we can scrape the whole thing. Let's try to build that loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do this in class. Let's also tackle some other scraping examples you may have. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selenium is a very powerful tool. Do note that browsers can now be run as \"headless\", meaning that you don't need a window to interact with them. While we've not used Selenium as headless in the class in order to keep an eye on what we're doing, you probably wouldn't do that when you run your giant scraping function on thousands of pages. That was one of the big upside of selenium compared to a browser itself. In the future, we'll probably use browsers and no longer emulation to scrape!\n",
    "\n",
    "Do note that in spite of your best efforts, scraping code is fragile at best. Website structures change as content is added do them or they're being revamped. Before you invest a lot of time in scraping, make sure 1- there's no other way to get the data and 2- you're going to be able to be flexible enough. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
